//TRUST_NO_INDENT
/****************************************************************************
* Copyright (c) 2015 - 2016, CEA
* All rights reserved.
*
* Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
*
* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
* OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*
*****************************************************************************/
/////////////////////////////////////////////////////////////////////////////
//
// File      : simd_tools_avx.h
// Directory : $IJK_ROOT/src/IJK/solveur_mg
//
/////////////////////////////////////////////////////////////////////////////
//
// WARNING: DO NOT EDIT THIS FILE! Only edit the template file simd_tools_avx.h.P
//
// This is an Intel AVX implementation of the SimdFloat SimdDouble etc... classes
// (requires -mavx flag on gcc)
// Simd vector types are of size 32 bytes (8 floats or 4 doubles).
#ifndef simd_tools_avx_included
#define simd_tools_avx_included

#include <immintrin.h>
#include <assert.h>
#include <stdint.h>

// ToDo: remove it! Temporary way to respect trust rules (name of the file = name of the class)
   class simd_tools_avx
   {
public:
     simd_tools_avx() {}
    ~simd_tools_avx() {}
   };
   
/*! @brief returns the size in bytes of SIMD vectors on the current architecture (for memory alignment).
 *
 * Intel AVX: 8 floats or 4 doubles
 *
 */
inline int simd_getalign()
{
  return 32;
}
/*! @brief allocates a memory bloc of give size (in bytes) with proper alignment for SIMD.
 *
 */
inline void * simd_malloc (size_t size)
{
  return _mm_malloc(size, simd_getalign());
}
/*! @brief frees a memory bloc previously allocated with simd_malloc()
 *
 */
inline void simd_free(void * ptr)
{
  _mm_free(ptr);
}

// uintptr_t should be defined in stdint.h
//  (this type is the result of pointer operations like ptr1-ptr2)
typedef uintptr_t uintptr_type;
/*! @brief returns 1 if pointer is aligned on size bytes, 0 otherwise Warn: size must be a power of 2.
 *
 */
inline int aligned(const void *ptr, int size)
{
  return ((uintptr_type)ptr & (uintptr_type)(size-1)) == 0;
}

#define _SimdAligned_ __attribute__ ((aligned (32)))

// Implementation for single precision type
#define Simd_floatSIZE 8

/*! @brief This class provides a generic access to simd operations on IA32 and Intel 64 architecture.
 *
 * Functionalities provided by the class are designed to match those provided by common
 *  processor architectures (Altivec, SSE, etc):
 *   - load vector size aligned data from memory (SimdGet)
 *   - getting x[i-1] and x[i+1] efficiently for finite difference algorithms
 *     (SimdGetAtLeft, SimdGetAtRight, etc)
 *   - arithmetic operations (+ - * /)
 *   - conditional affectation (SimdSelect)
 *  See simd_malloc() and simd_free() to allocate aligned blocs of memory.
 *
 */
class Simd_float
{
public:
  typedef float value_type;
  
  Simd_float() {};
  // Size of the vector (depends on the architecture and scalar type)
  static int size() {
    return 8;
  }

  void operator+=(Simd_float a) {
    data_ = _mm256_add_ps(data_, a.data_);
  }
  void operator*=(Simd_float a) {
    data_ = _mm256_mul_ps(data_, a.data_);
  }
  void operator-=(Simd_float a) {
    data_ = _mm256_sub_ps(data_, a.data_);
  }

  // The type below is architecture specific.
  // Code using it will be non portable.
  __m256 data_;
  // Commodity default constructor (provides implicit conversion)
  Simd_float(__m256 x) : data_(x) {};
  Simd_float(float x) { data_ = _mm256_broadcast_ss(&x); }
 
};

/*! @brief Returns the vector found at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline Simd_float SimdGet(const float *data)
{
  return _mm256_load_ps(data);
}

/*! @brief Stores vector x at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline void SimdPut(float *data, Simd_float x)
{
  _mm256_store_ps(data, x.data_);
}

/*! @brief Returns the vector x starting at adress data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation.
 *
 */
inline Simd_float SimdGetAtRight(const float *data)
{
  return _mm256_loadu_ps(data+1);
}

/*! @brief Returns the vector x starting at adress data-1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation.
 *
 */
inline Simd_float SimdGetAtLeft(const float *data)
{
  return _mm256_loadu_ps(data-1);
}

/*! @brief Returns the vector left and center starting at adress data-1 and data data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation
 *
 */
inline void SimdGetLeftCenter(const float *data, Simd_float &left, Simd_float &center)
{
  // codage utilisant une instruction load non alignee et une instruction alignee
  // (moins efficace)
  left = _mm256_loadu_ps(data-1);
  center = _mm256_load_ps(data);
}

/*! @brief Returns the vector center and right starting at adress data and data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation
 *
 */
inline void SimdGetCenterRight(const float *data, 
				   Simd_float &center,
				   Simd_float &right)
{
  center = _mm256_load_ps(data);
  right = _mm256_loadu_ps(data+1);
}

/*! @brief Returns the vectors left, center and right starting at adress data-1, data and data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs three vector loads and two shift operations
 *
 */
inline void SimdGetLeftCenterRight(const float *data, 
				   Simd_float &left,
				   Simd_float &center,
				   Simd_float &right)
{
  left = _mm256_loadu_ps(data-1);
  center = _mm256_load_ps(data);
  right = _mm256_loadu_ps(data+1);
}

inline void SimdGetLeftleftLeftCenterRight(const float *data, 
					   Simd_float &leftleft,
					   Simd_float &left,
					   Simd_float &center,
					   Simd_float &right)
{
  Process::exit(); // GF A verifier
  leftleft = _mm256_loadu_ps(data-2);
  left =   _mm256_loadu_ps(data-1);
  center = _mm256_loadu_ps(data);
  right = _mm256_loadu_ps(data+1);
}


#if 0
/*! @brief this class optimizes the fetching of data at left and at right of an increasing pointer.
 *
 *   Example with float *A for arrays of size 4:
 *   SimdLeftCenterRightGetter obj(A); // prepares to fetch the values around A
 *   obj.fetch(A); // fetches A[-1]..A[2] in left, A[0]..A[3] in center and A[1]..A[4] in right
 *   A += 4;
 *   obj.fetch(A); // fetches values around A[4]..A[8]
 *
 */
class SimdLeftCenterRightGetter
{
  SimdLeftCenterRightGetter(const float *data);
  void shift_and_get(const float *data) {
    data_left_ = 
  }
  _VT_ left();
  _VT_ center();
  _VT_ right();
 protected:
  _m128 data_left_; // aligned value at left
  _m128 data_center1_; // aligned value at center
  _m128 data_center2_;
  _m128 data_right_;
};
#endif

/*! @brief returns a+b
 *
 */
inline Simd_float operator+(Simd_float a, Simd_float b)
{
  return _mm256_add_ps(a.data_, b.data_);
}

/*! @brief returns a-b
 *
 */
inline Simd_float operator-(Simd_float a, Simd_float b)
{
  return _mm256_sub_ps(a.data_, b.data_);
}

/*! @brief returns a*b
 *
 */
inline Simd_float operator*(Simd_float a, Simd_float b)
{
  return _mm256_mul_ps(a.data_, b.data_);
}


/*! @brief This function performs the following operation on the vectors for (i=0; i<size())
 *
 *    if (x1[i] < x2[i])
 *      result[i] = value_if_x1_lower_than_x2[i]
 *    else
 *      result[i] = value_otherwise[i]
 *
 */
inline Simd_float SimdSelect(Simd_float x1,
			       Simd_float x2,
			       Simd_float value_if_x1_lower_than_x2,
			       Simd_float value_otherwise)
{

  __m256 compare = _mm256_sub_ps(x1.data_, x2.data_); // compare = x1 - x2
  // if sign bit of compare is 1 (x1-x2 < 0), take "value if x1 lower than x2", otherwise take "value otherwise"
  __m256 resu = _mm256_blendv_ps(value_otherwise.data_, value_if_x1_lower_than_x2.data_, compare);
  return resu;
}

// Returns a vector built with min(a[i],b[i]) (element wise)
inline Simd_float SimdMin(const Simd_float & a, const Simd_float  & b)
{
  return _mm256_min_ps(a.data_, b.data_);
}

// Returns a vector built with max(a[i],b[i]) (element wise)
inline Simd_float SimdMax(const Simd_float & a, const Simd_float  & b)
{
  return _mm256_max_ps(a.data_, b.data_);
}

// Returns a 12 bits accurate result of a/b
inline Simd_float SimdDivideLow(const Simd_float & a, const Simd_float & b)
{
  return _mm256_rcp_ps(b.data_) * a.data_;
}

// Returns a 22 bits accurate result of a/b
inline Simd_float SimdDivideMed(const Simd_float & a, const Simd_float & b)
{
  __m256 x = _mm256_rcp_ps(b.data_); // x = approximation de 1/b
  __m256 y = _mm256_mul_ps(a.data_, x); // y = a * x
  // tmp = (- b * y + a)
  // resu = tmp * x + y
  __m256 tmp = _mm256_sub_ps(a.data_, _mm256_mul_ps(b.data_, y));
  __m256 resu = _mm256_add_ps(_mm256_mul_ps(tmp, x), y);

  // The following code is for fma capable hardware (not tested !)
  // __m256 tmp = _mm256_fnmadd_ps(b.data_, y, a.data_);
  // __m256 resu = _mm267_fmadd_ps(tmp, x, y);

  return resu;
}

// Returns a 22 bits accurate result of 1/b
inline Simd_float SimdReciprocalMed(const Simd_float & b)
{
  __m256 x = _mm256_rcp_ps(b.data_); // x = approximation de 1/b
  // resu = (2 - b * x) * x
  __m256 resu = _mm256_mul_ps(_mm256_sub_ps(Simd_float(2.f).data_, _mm256_mul_ps(b.data_, x)), x);
  return resu;
}

// Implementation for double precision type
#define Simd_doubleSIZE 4

/*! @brief This class provides a generic access to simd operations on IA32 and Intel 64 architecture.
 *
 * Functionalities provided by the class are designed to match those provided by common
 *  processor architectures (Altivec, SSE, etc):
 *   - load vector size aligned data from memory (SimdGet)
 *   - getting x[i-1] and x[i+1] efficiently for finite difference algorithms
 *     (SimdGetAtLeft, SimdGetAtRight, etc)
 *   - arithmetic operations (+ - * /)
 *   - conditional affectation (SimdSelect)
 *  See simd_malloc() and simd_free() to allocate aligned blocs of memory.
 *
 */
class Simd_double
{
public:
  typedef double value_type;
  
  Simd_double() {};
  // Size of the vector (depends on the architecture and scalar type)
  static int size() {
    return 4;
  }

  void operator+=(Simd_double a) {
    data_ = _mm256_add_pd(data_, a.data_);
  }
  void operator*=(Simd_double a) {
    data_ = _mm256_mul_pd(data_, a.data_);
  }
  void operator-=(Simd_double a) {
    data_ = _mm256_sub_pd(data_, a.data_);
  }

  // The type below is architecture specific.
  // Code using it will be non portable.
  __m256d data_;
  // Commodity default constructor (provides implicit conversion)
  Simd_double(__m256d x) : data_(x) {};
  Simd_double(double x) { data_ = _mm256_broadcast_sd(&x); }
 
};

/*! @brief Returns the vector found at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline Simd_double SimdGet(const double *data)
{
  return _mm256_load_pd(data);
}

/*! @brief Stores vector x at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline void SimdPut(double *data, Simd_double x)
{
  _mm256_store_pd(data, x.data_);
}

/*! @brief Returns the vector x starting at adress data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation.
 *
 */
inline Simd_double SimdGetAtRight(const double *data)
{
  return _mm256_loadu_pd(data+1);
}

/*! @brief Returns the vector x starting at adress data-1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation.
 *
 */
inline Simd_double SimdGetAtLeft(const double *data)
{
  return _mm256_loadu_pd(data-1);
}

/*! @brief Returns the vector left and center starting at adress data-1 and data data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation
 *
 */
inline void SimdGetLeftCenter(const double *data, Simd_double &left, Simd_double &center)
{
  // codage utilisant une instruction load non alignee et une instruction alignee
  // (moins efficace)
  left = _mm256_loadu_pd(data-1);
  center = _mm256_load_pd(data);
}

/*! @brief Returns the vector center and right starting at adress data and data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs two vector loads and a shift operation
 *
 */
inline void SimdGetCenterRight(const double *data, 
				   Simd_double &center,
				   Simd_double &right)
{
  center = _mm256_load_pd(data);
  right = _mm256_loadu_pd(data+1);
}

/*! @brief Returns the vectors left, center and right starting at adress data-1, data and data+1 data must be aligned for the architecture (see simd_malloc())
 *
 *   The implementation usually needs three vector loads and two shift operations
 *
 */
inline void SimdGetLeftCenterRight(const double *data, 
				   Simd_double &left,
				   Simd_double &center,
				   Simd_double &right)
{
  left = _mm256_loadu_pd(data-1);
  center = _mm256_load_pd(data);
  right = _mm256_loadu_pd(data+1);
}

inline void SimdGetLeftleftLeftCenterRight(const double *data, 
					   Simd_double &leftleft,
					   Simd_double &left,
					   Simd_double &center,
					   Simd_double &right)
{
  Process::exit(); // GF A verifier
  leftleft = _mm256_loadu_pd(data-2);
  left =   _mm256_loadu_pd(data-1);
  center = _mm256_loadu_pd(data);
  right = _mm256_loadu_pd(data+1);
}


#if 0
/*! @brief this class optimizes the fetching of data at left and at right of an increasing pointer.
 *
 *   Example with float *A for arrays of size 4:
 *   SimdLeftCenterRightGetter obj(A); // prepares to fetch the values around A
 *   obj.fetch(A); // fetches A[-1]..A[2] in left, A[0]..A[3] in center and A[1]..A[4] in right
 *   A += 4;
 *   obj.fetch(A); // fetches values around A[4]..A[8]
 *
 */
class SimdLeftCenterRightGetter
{
  SimdLeftCenterRightGetter(const double *data);
  void shift_and_get(const double *data) {
    data_left_ = 
  }
  _VT_ left();
  _VT_ center();
  _VT_ right();
 protected:
  _m128 data_left_; // aligned value at left
  _m128 data_center1_; // aligned value at center
  _m128 data_center2_;
  _m128 data_right_;
};
#endif

/*! @brief returns a+b
 *
 */
inline Simd_double operator+(Simd_double a, Simd_double b)
{
  return _mm256_add_pd(a.data_, b.data_);
}

/*! @brief returns a-b
 *
 */
inline Simd_double operator-(Simd_double a, Simd_double b)
{
  return _mm256_sub_pd(a.data_, b.data_);
}

/*! @brief returns a*b
 *
 */
inline Simd_double operator*(Simd_double a, Simd_double b)
{
  return _mm256_mul_pd(a.data_, b.data_);
}


/*! @brief This function performs the following operation on the vectors for (i=0; i<size())
 *
 *    if (x1[i] < x2[i])
 *      result[i] = value_if_x1_lower_than_x2[i]
 *    else
 *      result[i] = value_otherwise[i]
 *
 */
inline Simd_double SimdSelect(Simd_double x1,
			       Simd_double x2,
			       Simd_double value_if_x1_lower_than_x2,
			       Simd_double value_otherwise)
{

  __m256d compare = _mm256_sub_pd(x1.data_, x2.data_); // compare = x1 - x2
  // if sign bit of compare is 1 (x1-x2 < 0), take "value if x1 lower than x2", otherwise take "value otherwise"
  __m256d resu = _mm256_blendv_pd(value_otherwise.data_, value_if_x1_lower_than_x2.data_, compare);
  return resu;
}

// Returns a vector built with min(a[i],b[i]) (element wise)
inline Simd_double SimdMin(const Simd_double & a, const Simd_double  & b)
{
  return _mm256_min_pd(a.data_, b.data_);
}

// Returns a vector built with max(a[i],b[i]) (element wise)
inline Simd_double SimdMax(const Simd_double & a, const Simd_double  & b)
{
  return _mm256_max_pd(a.data_, b.data_);
}

// Returns a 22 bits accurate result of a/b
inline Simd_double SimdDivideMed(const Simd_double & a, const Simd_double & b)
{
  // Not optimized: returns a full precision result
  return _mm256_div_pd(a.data_, b.data_);
}

inline Simd_double SimdReciprocalMed(const Simd_double & b)
{
  Simd_double one(1.);
  return _mm256_div_pd(one.data_, b.data_);
}


class Simd_int
{
public:
  typedef int value_type;
 
  Simd_int() {};
  static int size() {
    return 8;
  }
/*   void operator&=(Simd_int a) { */
/*     data_ = _mm_and_si128(data_, a.data_); */
/*   } */
  void operator|=(Simd_int a) {
    data_ = _mm256_castps_si256(_mm256_or_ps(_mm256_castsi256_ps(data_), _mm256_castsi256_ps(a.data_)));
  }

  // The type below is architecture specific.
  // Code using it will be non portable.
  __m256i data_;

  // Commodity default constructor (provides implicit conversion)
  Simd_int(__m256i x) : data_(x) {};
    Simd_int(int x) { data_ = _mm256_set1_epi32(x); }
};

/*! @brief Returns the vector found at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline Simd_int SimdGet(const int *data)
{
  return _mm256_load_si256((__m256i const *)data);
}

/*! @brief Stores vector x at address data.
 *
 * data must be aligned for the architecture (see simd_malloc())
 *
 */
inline void SimdPut(int *data, Simd_int x)
{
  _mm256_store_si256((__m256i*)data, x.data_);
}
#if 0
// Returns 0 if x1!=x2 and value_if_equal if x1==x2
inline Simd_int SimdTestEqual(Simd_float x1, Simd_float x2, Simd_int value_if_equal)
{
  return _mm_and_si128(_mm_castps_si128(_mm_cmpeq_ps(x1.data_, x2.data_)), value_if_equal.data_);
}

inline Simd_int SimdTestEqual(Simd_float x1, Simd_float x2, 
			      Simd_int value_if_equal, Simd_int value_if_not_equal)
{
  __m128i compare = _mm_castps_si128(_mm_cmpeq_ps(x1.data_, x2.data_));
  return _mm_or_si128(_mm_and_si128(compare, value_if_equal.data_),
		      _mm_andnot_si128(compare, value_if_not_equal.data_));
}
#endif

inline Simd_int SimdSelect(Simd_float x1,
			   Simd_float x2,
			   Simd_int value_if_x1_lower_than_x2,
			   Simd_int value_otherwise)
{
  __m256 compare = _mm256_sub_ps(x1.data_, x2.data_); // compare = x1 - x2
  // if sign bit of compare is 1 (x1-x2 < 0), take "value if x1 lower than x2", otherwise take "value otherwise"
  
  __m256 resu = _mm256_blendv_ps(_mm256_castsi256_ps(value_otherwise.data_), 
				 _mm256_castsi256_ps(value_if_x1_lower_than_x2.data_), 
				 compare);
  return _mm256_castps_si256(resu);
}

inline void SimdCompareAndSetIfLower(const Simd_float & x_new, Simd_float & x, 
				     const Simd_int & i_new, Simd_int & i)
{
  __m256 compare = _mm256_sub_ps(x_new.data_, x.data_); // compare = x_new - x
  x = _mm256_blendv_ps(x.data_, x_new.data_, compare);
  i = _mm256_castps_si256(_mm256_blendv_ps(_mm256_castsi256_ps(i.data_), 
					   _mm256_castsi256_ps(i_new.data_), 
					   compare));
  // The code does this for each vector element:
  //if (x_new < x) {
  //  x = x_new;
  //  i = i_new;
  //}
}
#endif


