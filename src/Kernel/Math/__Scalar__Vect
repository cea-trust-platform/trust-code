#Pmacro declareVect(__Scalar__,__scalar__)

#ifndef __Scalar__Vect_included
#define __Scalar__Vect_included

#include <TRUSTArray.h>
#include <MD_Vector.h>
 
// A nettoyer: les includes suivants ne sont pas necessaires a ce fichier
//  mais a d'autres qui ne font pas les include
#include <Vect.h>
#Pif("__scalar__"=="double")
#include <IntVect.h>
#Pendif

class __Scalar__Vect : public TRUSTArray<__scalar__>
{
  Declare_instanciable_sans_constructeur_ni_destructeur(__Scalar__Vect);
public:
  __Scalar__Vect() : size_reelle_(0), line_size_(1) {};
  __Scalar__Vect(int n);
//  __Scalar__Vect(int n, __scalar__ x);
  __Scalar__Vect(const __Scalar__Vect&);
  // Pas de constructeur par copie de ArrOf__Scalar__, voir __Scalar__Vect(const __Scalar__Vect&)
  ~__Scalar__Vect() {};
  virtual void reset();
  virtual void detach_vect();

  __Scalar__Vect& operator=(const __Scalar__Vect&);
  __Scalar__Vect& operator=(__scalar__);

  virtual void ref(const __Scalar__Vect &);
  virtual void ref_data(__scalar__* ptr, int new_size); 
  virtual void ref_array(ArrOf__Scalar__ &, int start = 0, int sz = -1);

  inline void resize(int, Array_base::Resize_Options opt = COPY_INIT);
  virtual void resize_tab(int n, Array_base::Resize_Options opt = COPY_INIT);
  void copy(const ArrOf__Scalar__ &, Array_base::Resize_Options opt = COPY_INIT);
  void copy(const __Scalar__Vect &, Array_base::Resize_Options opt = COPY_INIT);

  // par defaut: min et max sur items reels (compat. 1.5.6):
  __scalar__ local_max_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ local_min_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ local_max_abs_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ local_min_abs_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ mp_max_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ mp_min_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ mp_max_abs_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
  __scalar__ mp_min_abs_vect(Mp_vect_options opt = VECT_REAL_ITEMS) const;
#Pif("__scalar__"=="double")
  __scalar__ mp_norme_vect() const;
#Pendif

  void operator+=(const __Scalar__Vect&);
  void operator+=(const __scalar__);
  void operator-=(const __Scalar__Vect&);
  void operator-=(const __scalar__);
  void operator*=(const __Scalar__Vect&);
  void operator*= (const __scalar__);
#Pif("__scalar__"=="double")
  void operator/=(const __Scalar__Vect&);
  void operator/= (const __scalar__); 

  // Options par defaut choisies pour compatibilite avec la version precedente
  // Attention: il y avait un echange_espace_virtuel avant, ce n'est pas strictement equivalent
  void abs(Mp_vect_options opt = VECT_ALL_ITEMS);
  void   carre(Mp_vect_options opt = VECT_ALL_ITEMS);
  void   racine_carree(Mp_vect_options opt = VECT_ALL_ITEMS);
  void ajoute(double alpha, const DoubleVect& y, Mp_vect_options opt = VECT_ALL_ITEMS); // x+=alpha*y
  void ajoute_sans_ech_esp_virt(double alpha, const DoubleVect& y, Mp_vect_options opt = VECT_REAL_ITEMS); // x+=alpha*y sans echange_espace_virtuel
  void ajoute_produit_scalaire(double alpha, const DoubleVect&, const DoubleVect&, Mp_vect_options opt = VECT_ALL_ITEMS); // z+=alpha*x*y;
  void ajoute_carre(double alpha, const DoubleVect& y, Mp_vect_options opt = VECT_ALL_ITEMS); 
#Pendif

  inline int size() const;
  inline int size_totale() const;
  inline int size_reelle() const;
  inline int size_reelle_ok() const;
  inline int line_size() const;

  virtual void echange_espace_virtuel();
  
  virtual const MD_Vector & get_md_vector() const { return md_vector_; }
  virtual void              set_md_vector(const MD_Vector &);
  virtual void jump(Entree &);
  virtual void lit(Entree &, int resize_and_read=1);
  virtual void ecrit(Sortie &) const;

protected:
  inline void set_line_size_(int n);
  inline void resize_vect_(int n, Array_base::Resize_Options opt = COPY_INIT);
  void copy_(const __Scalar__Vect & v, Array_base::Resize_Options opt = COPY_INIT);
  void attach_vect(const __Scalar__Vect & v, int start, int size = -1);
  //void detach_vect(); // Not used in protected mode => Need in public mode => So method moved
private:
  // Un __Scalar__Vect est un ArrOf__Scalar__ qui possede eventuellement une structure de tableau
  // distribue. Ce pointeur peut etre nul.
  MD_Vector md_vector_;
  // Propriete size_reelle du tableau (fournie par scattered_vect_data)
  // -1 => l'appel a size_reelle() et size() est invalide pour ce vecteur.
  int size_reelle_;
  // Facteur multiplicatif a appliquer entre md_vector_.nb_items_tot() et size_array()
  //  et entre md_vector_.nb_items_reels() et size_reelle_.
  // Si l'objet est un tableau, ce facteur est generalement egal au produit
  //  des dimension(i) pour i>1 (une ligne du tableau par item geometrique du descripteur)
  // Attention, line_size_ peut etre nul pour un tableau a zero colonnes mais pas s'il y a un descripteur
  //  attache.
  int line_size_;
};

// Description: Taille de l'espace "reel" du vecteur.
//  (si md_vector_ est nul, cette valeur est identique a size_array(),
//   sinon, soit elle est egale a md_vector_....get_nb_items(),
//          soit l'appel est invalide (en particulier si les items reels ne
//           sont pas regroupes en debut de tableau)
//  On peut interroger le vecteur pour savoir si size_reelle() est valide avec size_reelle_ok()
inline int __Scalar__Vect::size_reelle() const
{
  // Si cet assert plante, c'est que l'appel a ete declare invalide par le
  //  MD_Vect associe a ce vecteur.
  assert(size_reelle_ >= 0);
  // Si cet assert plante, c'est que le tableau a ete redimensionne avec resize_array() au lieu de resize().
  // (invalide pour un Vect ou un Tab).
  assert(size_array() == size_reelle_ || md_vector_.non_nul());
  return size_reelle_;
}

// Description renvoie 1 si l'appel a size() et size_reelle() est valide, 0 sinon
inline int __Scalar__Vect::size_reelle_ok() const
{
  return size_reelle_ >= 0;
}

// Description: Identique a size_reelle()
inline int __Scalar__Vect::size() const
{
  return size_reelle();
}

// Description: Identique a size_array()
inline int __Scalar__Vect::size_totale() const
{
  return size_array();
}

inline int __Scalar__Vect::line_size() const
{
  // Si line_size_ est nulle, size_array doit etre nul aussi
  assert(line_size_ > 0 || size_array() == 0);
  return line_size_;
}

// Description: change l'attribut line_size_ du tableau avec n >= 1
//  n == 0 est autorise uniquement si size_array_ == 0
// Precondition: le md_vector_ doit etre nul (il faut attribuer md_vector_
//  apres la line_size_ car lorsqu'on attribue md_vector_ on teste
//  la validite des tailles de tableaux en fonction de line_size_)
//  ou la line_size_ ne doit pas changer (cas d'un resize qui ne change rien)
inline void __Scalar__Vect::set_line_size_(int n)
{
  assert(!md_vector_.non_nul() || line_size_ == n);
  assert(n >= 0);
  line_size_ = n;
}

// Description: Change la taille du vecteur (identique a resize_array() 
//  pour le traitement des anciennes valeurs et de nouvelles cases).
//  Attention: Cette methode n'est pas virtuelle, et afin d'eviter d'amener
//  un __Scalar__Tab dans un etat invalide, l'appel est interdit si l'objet
//  est de ce type. Dans ce cas, voir resize_tab.
// Precondition: l'appel est interdit si le vecteur a une structure parallele.
//  Le vecteur doit etre "resizable" (voir preconditions de ArrOf__Scalar__::resize_array()).
//  Appel interdit si l'objet n'est pas un __Scalar__Vect (sinon mauvaise initialisation
//  des dimensions du tableau)
inline void __Scalar__Vect::resize(int n, Array_base::Resize_Options opt)
{
  // Verifie que l'objet est bien du type __Scalar__Vect
  assert(n == size_array() || get_info() == __Scalar__Vect::info());
  resize_vect_(n, opt);
}

// Description: Methode interne de resize (appellee par __Scalar__Tab::resize(...))
//  sans precondition sur le type de l'objet.
// Precondition: l'appel est interdit si le vecteur a une structure parallele.
//  Le vecteur doit etre "resizable" (voir preconditions de ArrOf__Scalar__::resize_array()).
//  n doit etre un multiple de line_size_
inline void __Scalar__Vect::resize_vect_(int n, Array_base::Resize_Options opt)
{
  // Note B.M.: j'aurais voulu interdire completement resize des qu'on a un descripteur
  //  mais il y en a partout dans le code (on resize les tableaux alors qu'ils ont deja 
  //  la bonne taille). Donc j'autorise si la taille ne change pas.
  //assert(!md_vector_.non_nul() || n == size_array());
  // PL: 1.7.0 is now strict about this point:
  if (md_vector_.non_nul()) 
  { 
     Cerr << "Resize of a distributed array is forbidden!" << finl;
     exit();
  }
  assert(n == 0 || (n > 0 && line_size_ > 0 && n % line_size_ == 0));
  resize_array_(n, opt);
  size_reelle_ = n;
  // ne pas mettre line_size_ a 1 ici, voir __Scalar__Tab::resize_dim0()
}

// Fonctions non membres de la classe __Scalar__Vect

//int operator<(const __Scalar__Vect&, const __Scalar__Vect&);
//int operator>(const __Scalar__Vect& x, const __Scalar__Vect& y);
//int operator<=(const __Scalar__Vect& x, const __Scalar__Vect& y); 
//int operator>=(const __Scalar__Vect& x, const __Scalar__Vect& y);

int operator==(const __Scalar__Vect& x, const __Scalar__Vect& y);
int operator!=(const __Scalar__Vect& x, const __Scalar__Vect& y);
   
// Arithmetique :
//__Scalar__Vect operator+(const __Scalar__Vect&, const __scalar__);
//__Scalar__Vect operator-(const __Scalar__Vect&, const __scalar__);
//__Scalar__Vect operator-(const __Scalar__Vect&);

//void ordonne(__Scalar__Vect&);
   
//indice du min :
//int imin(const __Scalar__Vect&) ;
   
//indice du max
//int imax(const __Scalar__Vect&) ;
   
//valeur min
//__scalar__ min(const __Scalar__Vect&) ;
   
//valeur max
//__scalar__ max(const __Scalar__Vect&) ;

//produit scalaire :
//__scalar__ operator*(const __Scalar__Vect&, const __Scalar__Vect&);
//__Scalar__Vect operator * (const __Scalar__Vect&, __scalar__);
//__Scalar__Vect operator / (const __Scalar__Vect&, __scalar__);
//__Scalar__Vect operator * (__scalar__, const __Scalar__Vect&);

int local_imax_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ local_max_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
int local_imin_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ local_min_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ mp_max_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ mp_min_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
//__scalar__ mp_somme_vect_local(const __Scalar__Vect&);
__scalar__ mp_somme_vect(const __Scalar__Vect&);
__scalar__ local_max_abs_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ local_min_abs_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ mp_max_abs_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);
__scalar__ mp_min_abs_vect(const __Scalar__Vect &, Mp_vect_options opt = VECT_REAL_ITEMS);

// Valeurs par defaut choisies pour compatibilite approximative avec V1.5.6
// (compatibilite exacte non voulue car necessite echange_espace_virtuel)
void operator_add(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_add(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_sub(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_sub(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_multiply(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_multiply(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_negate(__Scalar__Vect & resu, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_egal(__Scalar__Vect & resu, __scalar__ x, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_egal(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);

#Pif("__scalar__"=="double")
__scalar__ local_prodscal(const __Scalar__Vect&, const __Scalar__Vect& );
__scalar__ mp_prodscal(const __Scalar__Vect&, const __Scalar__Vect& );
__scalar__ mp_norme_vect(const __Scalar__Vect&);
__scalar__ mp_carre_norme_vect(const __Scalar__Vect&);
__scalar__ local_carre_norme_vect(const DoubleVect & vx);
__scalar__ mp_moyenne_vect(const __Scalar__Vect &);
void ajoute_alpha_v(__Scalar__Vect & v, double alpha, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_REAL_ITEMS);
void ajoute_carre(__Scalar__Vect & v, double alpha, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void ajoute_produit_scalaire(__Scalar__Vect & v, double alpha, const __Scalar__Vect & vx, const __Scalar__Vect & vy, Mp_vect_options opt = VECT_ALL_ITEMS);
void racine_carree(__Scalar__Vect & v, Mp_vect_options opt = VECT_ALL_ITEMS);
void carre(__Scalar__Vect & v, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_inverse(__Scalar__Vect & resu, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_abs(__Scalar__Vect & resu, Mp_vect_options opt = VECT_ALL_ITEMS);
void tab_multiply_any_shape(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void tab_divide_any_shape(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_divide(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt = VECT_ALL_ITEMS);
void operator_divide(__Scalar__Vect & resu, const double x, Mp_vect_options opt = VECT_ALL_ITEMS);
#Pendif

#endif
#Pendmacro(declareVect)
#Pmacro implementeVect(__Scalar__,__scalar__)
#include <__Scalar__Vect.h>
#Pif("__scalar__"=="double")
#include <math.h>
#Pendif
#ifdef SGI_
#include <bstring.h>
#endif
#include <MD_Vector_base.h>
#include <MD_Vector_tools.h>
#include <limits.h>
#include <communications.h>

// Pour la relecture des anciens fichiers de reprise, voir __Scalar__Vect::lit()
#include <DescStructure.h>

Implemente_instanciable_sans_constructeur_ni_destructeur(__Scalar__Vect, "__Scalar__Vect", ArrOf__Scalar__);

// Description: Ecriture d'un vecteur sequentiel (sans descripteur parallele)
//  Pour l'instant: erreur si presence d'un md_vector_ (sinon quoi faire ?)
//  Pour les vecteurs paralleles, utiliser une methode de sauvegarde/reprise
Sortie & __Scalar__Vect::printOn(Sortie& os) const 
{
  if (nproc() > 1 && md_vector_.non_nul()) {
    Cerr << "Error in __Scalar__Vect::printOn: try to print a parallel vector" << finl;
    exit();
  }        
  ArrOf__Scalar__::printOn(os);
  return os;
}

// Description: Lecture d'un vecteur sequentiel (comme un ArrOf__Scalar__)
//  Attention: appel invalide si le vecteur a un MD_Vector non nul.
//  (pour les vecteurs paralleles, utiliser une methode de sauvegarde/reprise)
Entree & __Scalar__Vect::readOn(Entree& is)
{
  if (md_vector_.non_nul()) {
    // Que veut-on faire si on lit dans un vecteur ayant deja une structure parallele ?
    Cerr << "Error in __Scalar__Vect::readOn: vector has a parallel structure" << finl;
    exit();
  }
  ArrOf__Scalar__::readOn(is);
  size_reelle_ = size_array();
  line_size_ = 1;
  return is;
}

// Description: construction d'un vecteur de taille n.
//  Les elements du vecteur sont initialises a zero par defaut.
//  Pour ne pas initialiser les valeurs, utiliser ceci:
//   __Scalar__Vect toto;
//   toto.resize(n, NOCOPY_NOINIT);
__Scalar__Vect::__Scalar__Vect(int n) :
  ArrOf__Scalar__(n), 
  size_reelle_(n), 
  line_size_(1)
{
}
/*
// Description: construction d'un vecteur de taille n.
//  Les elements du vecteur sont initialises avec la valeur x.
__Scalar__Vect::__Scalar__Vect(int n, __scalar__ x) :  
  ArrOf__Scalar__(n, x), 
  size_reelle_(n), 
  line_size_(1)
{
}
*/
// Description: Constructeur par copie. Il s'agit d'un "deep copy" 
//   voir ArrOf__Scalar__::ArrOf__Scalar__(const ArrOf__Scalar__ &)
//  Remarque: il n'y a pas de constructeur par copie a partir de ArrOf__Scalar__
//   Ceci est volontaire, sinon on risque de grosses pertes de performances
//   par creation implicite d'objets, difficile a trouver.
//   (exemple: appel d'une methode toto(const IntVect &) avec un ArrOfInt
//    produit une copie du tableau !)
//  Utiliser copy() pour copier un ArrOf__Scalar__ dans un __Scalar__Vect
__Scalar__Vect::__Scalar__Vect(const __Scalar__Vect & v) :
  ArrOf__Scalar__(v), 
  md_vector_(v.md_vector_), 
  size_reelle_(v.size_reelle_), 
  line_size_(v.line_size_)
{
}

// Description: met l'objet dans l'etat obtenu par le constructeur par defaut.
//  (voir ArrOf__Scalar__::reset())
void __Scalar__Vect::reset()
{
  md_vector_.detach();
  line_size_ = 1;
  size_reelle_ = 0;
  ArrOf__Scalar__::reset();
}

// Description: detach the vector.
void __Scalar__Vect::detach_vect()
{
  md_vector_.detach();
}

// Description: detache le tableau et l'attache a v (sauf si v==*this, ne fait rien)
//  Fait pointer le tableau sur la meme zone de memoire que v et copie le MD_Vector
//  (utilise ArrOf__Scalar__::attach_array())
//  Attention, il devient alors interdit de resizer le tableau v ainsi que *this
//  Methode virtuelle reimplementee dans __Scalar__Tab
// Precondition:
//  L'objet ne doit pas etre un sous-type de __Scalar__Vect (sinon mauvaise initialisation
//  des dimensions.
void __Scalar__Vect::ref(const __Scalar__Vect & v)
{
  if (&v != this) {
    detach_array();
    attach_array(v);
    md_vector_ = v.md_vector_;
    size_reelle_ = v.size_reelle_;
    line_size_ = v.line_size_;
  }
}

// Description: copie la structure et les valeurs du tableau v dans *this avec ArrOf__Scalar__::operator=() 
//  (attention, si le tableau est de type ref_data ou ref, restrictions et cas particuliers !!!)
//  Attention: si on ne veut pas copier les structures paralleles, utiliser inject_array()
// Precondition: si le tableau *this doit etre resize, il doit etre de type __Scalar__Vect
//  (et pas d'un type derive !)
//  Si le tableau *this a deja une structure parallele, l'appel n'est autorise que les md_vector
//  sont deja identiques, sinon il faut d'abord faire un reset() du tableau (pour copier la structure),
//  ou utiliser inject_array() (pour ne pas copier la structure).
//  (ceci pour eviter d'ecraser accidentellement une structure parallele alors qu'on ne veut
//   que copier les valeurs).
__Scalar__Vect& __Scalar__Vect::operator=(const __Scalar__Vect & v)
{
  copy(v);
  return *this;
}

// Description: copie de la structure et des valeurs (si opt==COPY_INIT) du tableau v.
//  Attention, v doit vraiment etre de type ArrOf__Scalar__, pas d'un type derive
//  (sinon ambiguite: faut-il copier ou pas le MD_Vector ?)
// Precondition: Le vecteur ne doit pas avoir de structure de tableau distribue
//  et il doit vraiment etre de type __Scalar__Vect.
void __Scalar__Vect::copy(const ArrOf__Scalar__ & v, Array_base::Resize_Options opt)
{
  assert(get_info() == __Scalar__Vect::info());
  assert(v.get_info() == ArrOf__Scalar__::info());
  assert(!md_vector_.non_nul());
  resize(v.size_array(), opt);
  if (opt != NOCOPY_NOINIT)
    inject_array(v);
}

// Description: copie de la structure du vecteur v et des valeurs si opt==COPY_INIT.
// Precondition: idem que operator=(const __Scalar__Vect &)
void __Scalar__Vect::copy(const __Scalar__Vect & v, Array_base::Resize_Options opt)
{
  if (&v != this) {
    // Interdiction de resizer si l'objet est d'un type derive de __Scalar__Vect
    // (sinon mauvaise dimension(0) !)
    assert(v.size_array() == size_array() || get_info() == __Scalar__Vect::info());
    copy_(v, opt);
  }
}

// Description: methode protegee appelable depuis une classe derivee
//  (pas de precondition sur le type derive de *this)
void __Scalar__Vect::copy_(const __Scalar__Vect & v, Array_base::Resize_Options opt)
{
  assert(&v != this); // Il faut avoir fait le test avant !
  // Si le vecteur a deja une structure parallele, la copie n'est autorisee que si
  // le vecteur source a la meme structure. Si ce n'est pas le cas, utiliser inject_array()
  // pour copier uniquement les valeurs, ou faire d'abord reset() si on veut ecraser la structure.
  assert((!md_vector_.non_nul()) || (md_vector_ == v.md_vector_));
  resize_array_(v.size_array(), NOCOPY_NOINIT);
  if (opt != NOCOPY_NOINIT)
    inject_array(v);
  md_vector_ = v.md_vector_; // Pour le cas ou md_vector_ est nul et pas v.md_vector_
  size_reelle_ = v.size_reelle_;
  line_size_ = v.line_size_;
}

// Description: idem que ArrOf__Scalar__::operator=(__scalar__)
__Scalar__Vect& __Scalar__Vect::operator=(__scalar__ x)
{
  ArrOf__Scalar__::operator=(x);
  return *this;
}

// Description: methode virtuelle identique a resize_array(), permet de traiter
//   de facon generique les ArrOf, Vect et Tab.
//   Cree un tableau sequentiel...
//   Si l'objet est de type __Scalar__Vect, appel a resize(n)
// Prerequis: voir resize()
void __Scalar__Vect::resize_tab(int n, Array_base::Resize_Options opt)
{
  resize(n, opt);
}

// Description: voir ArrOf__Scalar__::ref_data().
//  (cree un tableau sans structure parallele)
void __Scalar__Vect::ref_data(__scalar__* ptr, int new_size)
{
  md_vector_.detach();
  ArrOf__Scalar__::ref_data(ptr, new_size);
  size_reelle_ = new_size;
  line_size_ = 1;
}

// Description: voir ArrOf__Scalar__::ref_array().
//  (cree un tableau sans structure parallele)
void __Scalar__Vect::ref_array(ArrOf__Scalar__& m, int start, int new_size)
{
  md_vector_.detach();
  ArrOf__Scalar__::ref_array(m, start, new_size);
  size_reelle_ = size_array(); // pas size qui peut valoir -1
  line_size_ = 1;
}

// Description: associe le md_vector au vecteur et initialise l'attribut size_reelle_
//  (voir methode size_reelle())
//  Si md_vector est nul, detache simplement le md_vector existant.
// Precondition: le vecteur doit deja avoir la taille appropriee au nouveau md_vector,
//  c'est a dire md_vector...get_nb_items_tot() * line_size_
void __Scalar__Vect::set_md_vector(const MD_Vector & md_vector)
{
  int size_r = size_array();
  if (md_vector.non_nul()) {
    size_r = md_vector.valeur().get_nb_items_reels();
    if (size_r >= 0)
      size_r *= line_size_;
    else
      size_r = -1; // Cas particulier ou la size_reelle ne veut rien dire
    int size_tot = md_vector.valeur().get_nb_items_tot() * line_size_;
    if (size_tot != size_array()) {
      Cerr << "Internal error in __Scalar__Vect::set_md_vector(): wrong array size\n"
           << " Needed size = " << md_vector.valeur().get_nb_items_tot() << " x " << line_size_
           << "\n Actual size = " << size_array() << finl;
      exit();
    }
    if (line_size_ == 0) {
      Cerr << "Internal error in __Scalar__Vect::set_md_vector():\n"
           << " cannot attach descriptor to empty array (line_size_ is zero)" << finl;
      exit();
    }
  }
  size_reelle_ = size_r;
  md_vector_ = md_vector;
}

void __Scalar__Vect::echange_espace_virtuel()
{
  MD_Vector_tools::echange_espace_virtuel(*this);
}

// Description: ecriture des valeurs du tableau "raw" sans structure parallele
void __Scalar__Vect::ecrit(Sortie & os) const
{
  ArrOf__Scalar__::printOn(os);
  os << (int)-1 << finl; // le marqueur -1 indique que c'est le nouveau format "ecrit", sans structure parallele
}

void __Scalar__Vect::jump(Entree & is)
{
    __Scalar__Vect::lit(is, 0 /* Do not resize&read the array */);
}

// Description: lecture d'un tableau pour reprise de calcul. On lit les valeurs "raw".
//  Attention, si le tableau n'est pas vide, il doit deja avoir la bonne 
//  taille et la bonne structure, sinon erreur !
// Parameter resize_and_read if the array is sized AND read (by default, yes)
void __Scalar__Vect::lit(Entree & is, int resize_and_read)
{
  int sz = -1;
  is >> sz;
  if (resize_and_read)
  {
     if (size_array() == 0 && (!get_md_vector().non_nul())) {
       resize(sz, NOCOPY_NOINIT);
     } else {
       if (sz != size_array()) {
	 // Si on cherche a relire un tableau de taille inconnue, le tableau doit
	 // etre reset() a l'entree. On n'aura pas la structure parallele du tableau !
	 Cerr << "Error in __Scalar__Vect::lit(Entree & is): array has already a structure with incorrect size" << finl;
	 exit();
       }
     }
     is.get(addr(), sz);
  }
  else
  {
     // May be slow if large chunks are read:
     // __Scalar__ tmp;
     //for (int i=0;i<sz;i++) is >> tmp;
     // So we bufferize:
     int buffer_size = min(sz,128000);
     ArrOf__Scalar__ tmp(buffer_size);
     while(sz>buffer_size)
     {
        is.get(tmp.addr(), buffer_size);
	sz-=buffer_size;
     }
     is.get(tmp.addr(), sz);
  }
  int sz_reel = -2;
  is >> sz_reel;
  if (sz_reel >= 0) {
    // Lecture de l'ancien format. Ignore les valeurs lues.
    int sz_virt; 
    is >> sz_virt;
    DescStructure toto;
    is >> toto;
#Pif("__scalar__"=="double")
    ArrOfInt it_communs;
    is >> it_communs;
    ArrOfInt it_communs_tot;
    is >> it_communs_tot;
#Pendif
  }
}

// Description:
//  renvoie 1 si meme strucuture parallele et egalite au sens ArrOf__Scalar__
//  (y compris espaces virtuels)
//  BM: faut-il etre aussi strict, comparer uniquement size() elements ?
int operator==(const __Scalar__Vect& x, const __Scalar__Vect& y)
{
  if (!(x.get_md_vector() == y.get_md_vector()))
    return 0;
  const ArrOf__Scalar__ & ax = x;
  const ArrOf__Scalar__ & ay = y;
  return ax == ay;
}

int operator!=(const __Scalar__Vect& x, const __Scalar__Vect& y)
{
  return !(x == y);
}


#Pif("__scalar__"=="double")
#ifdef MICROSOFT
#define HUGE_VALL 1e99
#endif
#Pset(MIN_SCALAR (-HUGE_VALL))
#Pset(MAX_SCALAR HUGE_VALL)
// INVALID_SCALAR is used to fill arrays when values are not computed
// (virtual space might not be computed by operators).
// The value below probably triggers errors on parallel test cases but
// does not prevent from doing "useless" computations with it.
#Pset(INVALID_SCALAR -987654.321)
#Pendif
#Pif("__scalar__"=="int")
#Pset(MIN_SCALAR INT_MIN;)
#Pset(MAX_SCALAR INT_MAX;)
#Pset(INVALID_SCALAR INT_MAX)
#Pendif

#ifndef NDEBUG 
static void invalidate_data(__Scalar__Vect & resu, Mp_vect_options opt)
{
  const __scalar__ invalid = INVALID_SCALAR;
  const MD_Vector & md = resu.get_md_vector();
  const int line_size = resu.line_size();
  if (opt == VECT_ALL_ITEMS || (!md.non_nul()))
    return; // no invalid values
  assert(opt == VECT_SEQUENTIAL_ITEMS || opt == VECT_REAL_ITEMS);
  const ArrOfInt & items_blocs = (opt == VECT_SEQUENTIAL_ITEMS) ? md.valeur().get_items_to_sum() : md.valeur().get_items_to_compute();
  const int blocs_size = items_blocs.size_array();
  int i = 0;
  for (int blocs_idx = 0; blocs_idx < blocs_size; blocs_idx += 2) {
    // process data until beginning of next bloc, or end of array
    const int bloc_end = line_size * items_blocs[blocs_idx];
    __scalar__ *ptr = resu.addr() + i;
    for (; i < bloc_end; i++)
      *(ptr++) = invalid;
    i = items_blocs[blocs_idx+1] * line_size;
  }
  // Process until end of vector
  const int bloc_end = resu.size_array();
  __scalar__ *ptr = resu.addr() + i;
  for (; i < bloc_end; i++)
    *(ptr++) = invalid;
}
#endif 


#Pmacro patternVect(INIT_MACRO,COMPUTE_MACRO,WITH_RESU,ANY_SHAPE,WITH_VX,WITH_VY,opt,RETURN_VALUE)
{
  INIT_MACRO
    // Master vect donne la structure de reference, les autres vecteurs
    // doivent avoir la meme structure.
#Pif (WITH_RESU=="with_resu")
    const __Scalar__Vect & master_vect = resu;
#Pelse
  const __Scalar__Vect & master_vect = vx;
#Pendif
  int line_size = master_vect.line_size();
#Pif (ANY_SHAPE=="any_shape")
  int line_size_vx = vx.line_size();
#Pendif
#Pif ("__scalar__"=="double")#Pset(ABS fabs)#Pelse#Pset(ABS abs)#Pendif
  const MD_Vector & md = master_vect.get_md_vector();
  const int vect_size_tot = master_vect.size_totale();
#Pif (WITH_VX=="with_vx")
#Pif (ANY_SHAPE=="any_shape")
  // Le line_size du vecteur resu doit etre un multiple du line_size du vecteur vx
  assert(line_size > 0 && line_size_vx > 0 && line_size % line_size_vx == 0);
  const int delta_line_size = line_size / line_size_vx;
  assert(vx.size_totale() * delta_line_size == vect_size_tot); // this test is necessary if md is null
#Pelse
  assert(vx.line_size() == line_size);
  assert(vx.size_totale() == vect_size_tot); // this test is necessary if md is null
#Pendif
  assert(vx.get_md_vector() == md);
#Pendif
#Pif (WITH_VY=="with_vy")
  assert(vy.line_size() == line_size);
  assert(vy.size_totale() == vect_size_tot); // this test is necessary if md is null
  assert(vy.get_md_vector() == md);
#Pendif
  // Determine blocs of data to process, depending on "opt"
  int nblocs_left = 1;
  int one_bloc[2];
  const int *bloc_ptr;
  if (opt != VECT_ALL_ITEMS && md.non_nul()) {
    assert(opt == VECT_SEQUENTIAL_ITEMS || opt == VECT_REAL_ITEMS);
    const ArrOfInt & items_blocs = (opt == VECT_SEQUENTIAL_ITEMS) ? md.valeur().get_items_to_sum() : md.valeur().get_items_to_compute();
    assert(items_blocs.size_array() % 2 == 0);
    nblocs_left = items_blocs.size_array() >> 1;
    bloc_ptr = items_blocs.addr();
  } else if (vect_size_tot > 0) {
    // attention, si vect_size_tot est nul, line_size a le droit d'etre nul
    // Compute all data, in the vector (including virtual data), build a big bloc:
    nblocs_left = 1;
    bloc_ptr = one_bloc;
    one_bloc[0] = 0;
    one_bloc[1] = vect_size_tot / line_size;
  } else {
    // raccourci pour les tableaux vides (evite le cas particulier line_size == 0)
    return RETURN_VALUE;
  }
#Pif (WITH_RESU=="with_resu")
  __scalar__ *resu_base = resu.addr();
#Pendif
#Pif (WITH_VX=="with_vx")
  const __scalar__ *x_base = vx.addr();
#Pendif
#Pif (WITH_VY=="with_vy")
  const __scalar__ *y_base = vy.addr();
#Pendif
  for (; nblocs_left; nblocs_left--) {
    // Get index of next bloc start:
#Pif (ANY_SHAPE=="any_shape")
    const int begin_bloc = (*(bloc_ptr++)) * line_size_vx;
    const int end_bloc = (*(bloc_ptr++)) * line_size_vx;
#Pelse
    const int begin_bloc = (*(bloc_ptr++)) * line_size;
    const int end_bloc = (*(bloc_ptr++)) * line_size;
#Pendif    assert(begin_bloc >= 0 && end_bloc <= vect_size_tot && end_bloc >= begin_bloc);
#Pif (WITH_RESU=="with_resu")
#Pif (ANY_SHAPE=="any_shape")
    __scalar__* resu_ptr = resu_base + begin_bloc * delta_line_size;
#Pelse
    __scalar__* resu_ptr = resu_base + begin_bloc;#Pendif#Pendif
#Pif (WITH_VX=="with_vx")
                                                    const __scalar__* x_ptr = x_base + begin_bloc;
#Pendif
#Pif (WITH_VY=="with_vy")
    const __scalar__* y_ptr = y_base + begin_bloc;
#Pendif
    int count = end_bloc - begin_bloc;
    for (; count; count--) {
#Pif (WITH_VX=="with_vx")
      const __scalar__ x = *x_ptr;
#Pendif#Pif (ANY_SHAPE=="any_shape")#Pif (WITH_VY=="with_vy")
#Perror "Error ANY_SHAPE + WITH_VY pas prevu"
#Pendif
      // Any shape: pour chaque item de vx, on a delta_line_size items de resu a traiter
      for(int count2 = delta_line_size; count2; count2--) {#Pif (WITH_RESU=="with_resu")
          __scalar__ & p_resu = *(resu_ptr++);
#Pendif
        COMPUTE_MACRO
          }
#Pelse
#Pif (WITH_VY=="with_vy")
      const __scalar__ y = *(y_ptr++);
#Pendif
#Pif (WITH_RESU=="with_resu")
      __scalar__ & p_resu = *(resu_ptr++);
#Pendif
      COMPUTE_MACRO      
#Pendif
#Pif (WITH_VX=="with_vx")
        x_ptr++;
#Pendif
    }
  }
#Pif (WITH_RESU=="with_resu")
  // In debug mode, put invalid values where data has not been computed
#ifndef NDEBUG 
  invalidate_data(resu, opt);
#endif
#Pendif
  return RETURN_VALUE; 
}
#Punset(ABS)
#Pendmacro(patternVect)


// Description:
//  Renvoie la somme des x[i]*y[i] pour les items sequentiels sur ce processeur.
//  (tous les items si pas de structure parallele, sinon voir MD_Vector_base::get_items_to_sum()






__scalar__ local_prodscal(const __Scalar__Vect & vx, const __Scalar__Vect & vy)
#Pusemacro(patternVect)(__scalar__ sum = 0;, sum += x * y;, "without_resu", "no_any_shape", "with_vx", "with_vy", VECT_SEQUENTIAL_ITEMS, sum)
  __scalar__ local_carre_norme_vect(const __Scalar__Vect & vx)
#Pusemacro(patternVect)( __scalar__ sum = 0;, sum += x * x;, "without_resu", "no_any_shape", "with_vx", "without_vy", VECT_SEQUENTIAL_ITEMS, sum)
    __scalar__ mp_carre_norme_vect(const __Scalar__Vect & vx)
{
  return Process::mp_sum(local_carre_norme_vect(vx));
}
__scalar__ local_somme_vect(const __Scalar__Vect & vx)
#Pusemacro(patternVect)( __scalar__ sum = 0;, sum += x;, "without_resu", "no_any_shape", "with_vx", "without_vy", VECT_SEQUENTIAL_ITEMS, sum)
  __scalar__ mp_somme_vect(const __Scalar__Vect & vx)
{
  __scalar__ x = local_somme_vect(vx);
  __scalar__ y = Process::mp_sum(x);
  return y;
}
int local_imin_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ min_val = MAX_SCALAR; int i_min = -1;, if (x < min_val) { i_min = x_ptr - x_base; min_val = x; }, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, i_min)
  __scalar__ local_min_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ min_val = MAX_SCALAR;, min_val = (x < min_val) ? x : min_val;, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, min_val)
   int local_imax_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ max_val = MIN_SCALAR; int i_max = -1;, if (x > max_val) { i_max = x_ptr - x_base; max_val = x; }, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, i_max)
      __scalar__ local_max_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ max_val = MIN_SCALAR;, max_val = (x > max_val) ? x : max_val;, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, max_val)
        __scalar__ local_max_abs_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ max_val = 0;, __scalar__ xx = ABS(x); max_val = (xx > max_val) ? xx : max_val;, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, max_val)
          __scalar__ local_min_abs_vect(const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( __scalar__ min_val = MAX_SCALAR;, __scalar__ xx = ABS(x); min_val = (xx < min_val) ? xx : min_val;, "without_resu", "no_any_shape", "with_vx", "without_vy", opt, min_val)
            void operator_abs(__Scalar__Vect & resu, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu = ABS(p_resu);, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
              void operator_add(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu += x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                void operator_add(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu += x;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                  void operator_sub(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu -= x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                    void operator_sub(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu -= x;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                      void operator_multiply(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu *= x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                        void operator_multiply(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu *= x;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                          void operator_negate(__Scalar__Vect & resu, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu = -p_resu;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                            void operator_egal(__Scalar__Vect & resu, __scalar__ x, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu = x;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                              void operator_egal(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu = x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
#Pif("__scalar__"=="double")
                                void ajoute_alpha_v(__Scalar__Vect & resu, __scalar__ alpha, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu += alpha * x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                                  void ajoute_carre(__Scalar__Vect & resu, __scalar__ alpha, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu += alpha * x * x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                                    void ajoute_produit_scalaire(__Scalar__Vect & resu, __scalar__ alpha, const __Scalar__Vect & vx, const __Scalar__Vect & vy, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu += alpha * x * y;, "with_resu", "no_any_shape", "with_vx", "with_vy", opt,)
                                      void racine_carree(__Scalar__Vect & resu, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu = sqrt(p_resu);, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                                        void carre(__Scalar__Vect & resu, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu *= p_resu;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
void operator_divide(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , if (x==0) { Cerr << "Divide by 0 in __Scalar__Vect::operator_divide()" << finl; Process::exit();};p_resu /= x;, "with_resu", "no_any_shape", "with_vx", "without_vy", opt,)
                                            void operator_inverse(__Scalar__Vect & resu, Mp_vect_options opt)
#Pusemacro(patternVect)( , if (p_resu==0) { Cerr << "Divide by 0 in __Scalar__Vect::operateur_inverse()" << finl; Process::exit();}; p_resu = 1. / p_resu;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)
                                              static void tab_multiply_any_shape_(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , p_resu *= x;, "with_resu", "any_shape", "with_vx", "without_vy", opt,)
                                                static void tab_divide_any_shape_(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
#Pusemacro(patternVect)( , if (x==0) { Cerr << "Divide by 0 in __Scalar__Vect::tab_divide_any_shape_()" << finl; Process::exit();}; p_resu *= (1. / x);, "with_resu", "any_shape", "with_vx", "without_vy", opt,)
                                                // Cette methode permettent de multiplier un tableau a plusieurs dimensions par un tableau
                                                //  de dimension inferieure (par exemple un tableau a trois composantes par un tableau a une composante).
                                                //  Chaque valeur du tableau vx est utilisee pour plusieurs items consecutifs du tableau resu
                                                //  (le nombre de fois est le rapport des line_size() des deux tableaux).
                                                //  resu.line_size() doit etre un multiple int de vx.line_size() et les descripteurs doivent etre identiques.
                                                //  Cas particulier: vx peut contenir une constante unique (size_array() == 1 et descripteur nul),
                                                //   dans ce cas c'est un simple produit par la constante
                                                  void tab_multiply_any_shape(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
{
  if (vx.size_array() == 1 && !vx.get_md_vector().non_nul()) {
    // Produit par une constante
    __scalar__ x = vx[0];
    operator_multiply(resu, x, opt);
  } else if (vx.line_size() == resu.line_size()) {
    // Produit membre a membre
    operator_multiply(resu, vx, opt);
  } else {
    // Cas general
    tab_multiply_any_shape_(resu, vx, opt);
  }
}
// Idem que tab_multiply_any_shape() mais avec une division
void tab_divide_any_shape(__Scalar__Vect & resu, const __Scalar__Vect & vx, Mp_vect_options opt)
{
  if (vx.size_array() == 1 && !vx.get_md_vector().non_nul()) {
    // Produit par une constante
    if (vx[0]==0)
    {
       Cerr << "Divide by 0 in __Scalar__Vect::tab_divide_any_shape()" << finl;
       Process::exit();
    }
    __scalar__ x = 1. / vx[0];
    operator_multiply(resu, x, opt);
  } else if (vx.line_size() == resu.line_size()) {
    // Produit membre a membre
    operator_divide(resu, vx, opt);
  } else {
    // Cas general
    tab_divide_any_shape_(resu, vx, opt);
  }
}
#Pendif
void operator_divide(__Scalar__Vect & resu, const __scalar__ x, Mp_vect_options opt)
#Pusemacro(patternVect)( , if(x==0.) { Cerr << "Error: divide by 0 in operator_divide." << finl;Process::exit();};p_resu /= x;, "with_resu", "no_any_shape", "without_vx", "without_vy", opt,)

  __scalar__ __Scalar__Vect::local_min_vect(Mp_vect_options opt) const
{
  return ::local_min_vect(*this, opt);
}
__scalar__ __Scalar__Vect::local_max_vect(Mp_vect_options opt) const
{
  return ::local_max_vect(*this, opt);
}
__scalar__ __Scalar__Vect::local_max_abs_vect(Mp_vect_options opt) const
{
  return ::local_max_abs_vect(*this, opt);
}
__scalar__ __Scalar__Vect::local_min_abs_vect(Mp_vect_options opt) const
{
  return ::local_min_abs_vect(*this, opt);
}
__scalar__ __Scalar__Vect::mp_max_vect(Mp_vect_options opt) const
{
  return ::mp_max_vect(*this, opt);
}
__scalar__ __Scalar__Vect::mp_min_vect(Mp_vect_options opt) const
{
  return ::mp_min_vect(*this, opt);
}
__scalar__ __Scalar__Vect::mp_max_abs_vect(Mp_vect_options opt) const
{
  return ::mp_max_abs_vect(*this, opt);
}
__scalar__ __Scalar__Vect::mp_min_abs_vect(Mp_vect_options opt) const
{
  return ::mp_min_abs_vect(*this, opt);
}
void __Scalar__Vect::operator+=(const __Scalar__Vect & v)
{
  operator_add(*this, v);
}
void __Scalar__Vect::operator-=(const __Scalar__Vect & v)
{
  operator_sub(*this, v);
}
void __Scalar__Vect::operator*=(const __Scalar__Vect & v)
{
  operator_multiply(*this, v);
}
void __Scalar__Vect::operator+=(const __scalar__ x)
{
  operator_add(*this, x);
}
void __Scalar__Vect::operator-=(const __scalar__ x)
{
  operator_sub(*this, x);
}
void __Scalar__Vect::operator*=(const __scalar__ x)
{
  operator_multiply(*this, x);
}
__scalar__ mp_max_vect(const __Scalar__Vect & x, Mp_vect_options opt)
{
  __scalar__ s = local_max_vect(x, opt);
  s =  #Pif("__scalar__"=="double")Process::#Pendifmp_max(s);
  return s;
}
__scalar__ mp_min_vect(const __Scalar__Vect & x, Mp_vect_options opt)
{
  __scalar__ s = local_min_vect(x, opt);
  s =  #Pif("__scalar__"=="double")Process::#Pendifmp_min(s);
  return s;
}
__scalar__ mp_max_abs_vect(const __Scalar__Vect & x, Mp_vect_options opt)
{
  __scalar__ s = local_max_abs_vect(x, opt);
  s = #Pif("__scalar__"=="double")Process::#Pendifmp_max(s);
  return s;
}
__scalar__ mp_min_abs_vect(const __Scalar__Vect & x, Mp_vect_options opt)
{
  __scalar__ s = local_min_abs_vect(x, opt);
  s = #Pif("__scalar__"=="double")Process::#Pendifmp_min(s);
  return s;
}
#Pif("__scalar__"=="double")
void __Scalar__Vect::operator/=(const __scalar__ x)
{
  operator_divide(*this, x);
}
void __Scalar__Vect::operator/=(const __Scalar__Vect & v)
{
  operator_divide(*this, v);
}
void __Scalar__Vect::abs(Mp_vect_options opt)
{
  operator_abs(*this, opt);
}
void __Scalar__Vect::carre(Mp_vect_options opt)
{
  ::carre(*this, opt);
}
void __Scalar__Vect::racine_carree(Mp_vect_options opt)
{
  ::racine_carree(*this, opt);
}
void __Scalar__Vect::ajoute(__scalar__ alpha, const __Scalar__Vect& y, Mp_vect_options opt)
{
  ajoute_alpha_v(*this, alpha, y, opt);
  if (opt == VECT_ALL_ITEMS)
    echange_espace_virtuel();
}
void __Scalar__Vect::ajoute_sans_ech_esp_virt(__scalar__ alpha, const __Scalar__Vect& y, Mp_vect_options opt)
{
  ajoute_alpha_v(*this, alpha, y, opt);
}
void __Scalar__Vect::ajoute_produit_scalaire(__scalar__ alpha, const __Scalar__Vect & x, const __Scalar__Vect & y, Mp_vect_options opt)
{
  ::ajoute_produit_scalaire(*this, alpha, x, y, opt);
}
void __Scalar__Vect::ajoute_carre(__scalar__ alpha, const __Scalar__Vect& y, Mp_vect_options opt)
{
  ::ajoute_carre(*this, alpha, y, opt);
}
__scalar__ mp_moyenne_vect(const __Scalar__Vect & x)
{
  __scalar__ s = mp_somme_vect(x);
  __scalar__ n;
  const MD_Vector & md = x.get_md_vector();
  if (md.non_nul())
    n = md.valeur().nb_items_seq_tot() * x.line_size();
  else
  {
    // Coding error: mp_moyenne_vect is used on a not distributed __Scalar__Vect !
    assert(Process::nproc()==1);
    n = x.size_totale();
  }
  return s / n;
}
#Pendif
__scalar__ mp_prodscal(const __Scalar__Vect & x, const __Scalar__Vect & y)
{
  return Process::mp_sum(local_prodscal(x, y));
}



#Pif("__scalar__"=="double")
__scalar__ __Scalar__Vect::mp_norme_vect() const
{
  return ::mp_norme_vect(*this);
}
__scalar__ mp_norme_vect(const __Scalar__Vect & vx)
{
  __scalar__ x = mp_carre_norme_vect(vx);
  x = sqrt(x);
  return x;
}

#Pendif
#Punset(MIN_SCALAR)
#Punset(MAX_SCALAR)
#Punset(INVALID_SCALAR)
#Pendmacro(implementeVect)
